"""Add detected_project_types to metadata_indexing_jobs

Revision ID: 4e6982618f6e
Revises: 6b55dcdbf53f
Create Date: 2025-04-07 21:17:23.537003

"""
from typing import Sequence, Union
from datetime import datetime
import uuid

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import sqlite

# revision identifiers, used by Alembic.
revision: str = '4e6982618f6e'
down_revision: Union[str, None] = '6b55dcdbf53f'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('metadata_resources',
    sa.Column('name', sa.String(), nullable=False),
    sa.Column('resource_type', sa.String(), nullable=False),
    sa.Column('path', sa.String(), nullable=True),
    sa.Column('description', sa.Text(), nullable=True),
    sa.Column('raw_data', sa.JSON(), nullable=True),
    sa.Column('sql_content', sa.Text(), nullable=True),
    sa.Column('source_name', sa.String(), nullable=True),
    sa.Column('database', sa.String(), nullable=True),
    sa.Column('schema', sa.String(), nullable=True),
    sa.Column('columns', sa.JSON(), nullable=True),
    sa.Column('depends_on', sa.JSON(), nullable=True),
    sa.Column('is_active', sa.Boolean(), nullable=False),
    sa.Column('last_synced_at', sa.DateTime(), nullable=True),
    sa.Column('data_source_id', sa.String(length=36), nullable=False),
    sa.Column('metadata_indexing_job_id', sa.String(length=36), nullable=True),
    sa.Column('id', sa.String(length=36), nullable=False),
    sa.Column('created_at', sa.DateTime(), nullable=True),
    sa.Column('updated_at', sa.DateTime(), nullable=True),
    sa.Column('deleted_at', sa.DateTime(), nullable=True),
    sa.ForeignKeyConstraint(['data_source_id'], ['data_sources.id'], ),
    sa.ForeignKeyConstraint(['metadata_indexing_job_id'], ['metadata_indexing_jobs.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    with op.batch_alter_table('metadata_resources', schema=None) as batch_op:
        batch_op.create_index(batch_op.f('ix_metadata_resources_id'), ['id'], unique=True)

    with op.batch_alter_table('metadata_indexing_jobs', schema=None) as batch_op:
        batch_op.add_column(sa.Column('detected_project_types', sa.JSON(), nullable=True))

    # ### Data Migration: Copy dbt_resources to metadata_resources ###
    connection = op.get_bind()
    
    # Check if dbt_resources table exists and has data
    inspector = sa.inspect(connection)
    if 'dbt_resources' in inspector.get_table_names():
        # Get count of dbt_resources to migrate
        result = connection.execute(sa.text("SELECT COUNT(*) FROM dbt_resources WHERE deleted_at IS NULL"))
        dbt_count = result.scalar()
        
        if dbt_count > 0:
            print(f"Migrating {dbt_count} dbt_resources to metadata_resources...")
            
            # Create placeholder indexing jobs for data sources that don't have them
            # First, get all unique data_source_ids from dbt_resources
            data_sources_result = connection.execute(sa.text("""
                SELECT DISTINCT data_source_id, organization_id 
                FROM dbt_resources 
                WHERE deleted_at IS NULL
            """))
            data_sources = data_sources_result.fetchall()
            
            # Create placeholder indexing jobs
            for data_source_id, organization_id in data_sources:
                # Check if indexing job already exists
                job_result = connection.execute(sa.text("""
                    SELECT id FROM metadata_indexing_jobs 
                    WHERE data_source_id = :data_source_id 
                    AND status = 'completed'
                    LIMIT 1
                """), {"data_source_id": data_source_id})
                existing_job = job_result.fetchone()
                
                if not existing_job:
                    # Create placeholder job for migrated data
                    job_id = str(uuid.uuid4())
                    current_time = datetime.utcnow()
                    connection.execute(sa.text("""
                        INSERT INTO metadata_indexing_jobs (
                            id, data_source_id, organization_id, job_type, status,
                            detected_project_types, started_at, completed_at,
                            total_resources, processed_resources, is_active,
                            created_at, updated_at
                        ) VALUES (
                            :job_id, :data_source_id, :organization_id, 'dbt', 'completed',
                            :detected_types, :started_at, :completed_at,
                            0, 0, true, :created_at, :updated_at
                        )
                    """), {
                        "job_id": job_id,
                        "data_source_id": data_source_id,
                        "organization_id": organization_id,
                        "detected_types": '["dbt"]',
                        "started_at": current_time,
                        "completed_at": current_time,
                        "created_at": current_time,
                        "updated_at": current_time
                    })
                else:
                    job_id = existing_job[0]
                
                # Migrate dbt_resources for this data source
                connection.execute(sa.text("""
                    INSERT INTO metadata_resources (
                        id, name, resource_type, path, description, raw_data,
                        sql_content, source_name, database, schema, columns,
                        depends_on, is_active, last_synced_at, data_source_id,
                        metadata_indexing_job_id, created_at, updated_at, deleted_at
                    )
                    SELECT 
                        id, name, 
                        CASE 
                            WHEN resource_type = 'model' THEN 'dbt_model'
                            WHEN resource_type = 'source' THEN 'dbt_source'
                            WHEN resource_type = 'seed' THEN 'dbt_seed'
                            WHEN resource_type = 'macro' THEN 'dbt_macro'
                            WHEN resource_type = 'test' THEN 'dbt_test'
                            WHEN resource_type = 'metric' THEN 'dbt_metric'
                            WHEN resource_type = 'exposure' THEN 'dbt_exposure'
                            ELSE CONCAT('dbt_', resource_type)
                        END as resource_type,
                        path, description, raw_data, sql_content, source_name,
                        database, schema, columns, depends_on, is_active,
                        last_synced_at, data_source_id, :job_id,
                        created_at, updated_at, deleted_at
                    FROM dbt_resources 
                    WHERE data_source_id = :data_source_id 
                    AND deleted_at IS NULL
                """), {"data_source_id": data_source_id, "job_id": job_id})
            
            print(f"Successfully migrated {dbt_count} dbt_resources to metadata_resources")

    # Drop the old dbt_resources table
    with op.batch_alter_table('dbt_resources', schema=None) as batch_op:
        batch_op.drop_index('ix_dbt_resources_id')

    op.drop_table('dbt_resources')

    # ### end Alembic commands ###


def downgrade() -> None:
    # ### Data Migration: Copy metadata_resources back to dbt_resources ###
    connection = op.get_bind()
    
    # Check if metadata_resources table exists and has dbt data
    inspector = sa.inspect(connection)
    if 'metadata_resources' in inspector.get_table_names():
        # Get count of dbt metadata_resources to migrate back
        result = connection.execute(sa.text("""
            SELECT COUNT(*) FROM metadata_resources 
            WHERE resource_type LIKE 'dbt_%' AND deleted_at IS NULL
        """))
        metadata_count = result.scalar()
        
        if metadata_count > 0:
            print(f"Migrating {metadata_count} dbt metadata_resources back to dbt_resources...")
            
            # Migrate dbt metadata_resources back to dbt_resources
            connection.execute(sa.text("""
                INSERT INTO dbt_resources (
                    id, name, resource_type, path, description, raw_data,
                    sql_content, source_name, database, schema, columns,
                    depends_on, is_active, last_synced_at, data_source_id,
                    metadata_indexing_job_id, created_at, updated_at, deleted_at
                )
                SELECT 
                    id, name, 
                    CASE 
                        WHEN resource_type = 'dbt_model' THEN 'model'
                        WHEN resource_type = 'dbt_source' THEN 'source'
                        WHEN resource_type = 'dbt_seed' THEN 'seed'
                        WHEN resource_type = 'dbt_macro' THEN 'macro'
                        WHEN resource_type = 'dbt_test' THEN 'test'
                        WHEN resource_type = 'dbt_metric' THEN 'metric'
                        WHEN resource_type = 'dbt_exposure' THEN 'exposure'
                        ELSE SUBSTRING(resource_type, 5) -- Remove 'dbt_' prefix
                    END as resource_type,
                    path, description, raw_data, sql_content, source_name,
                    database, schema, columns, depends_on, is_active,
                    last_synced_at, data_source_id, metadata_indexing_job_id,
                    created_at, updated_at, deleted_at
                FROM metadata_resources 
                WHERE resource_type LIKE 'dbt_%' AND deleted_at IS NULL
            """))
            
            print(f"Successfully migrated {metadata_count} dbt metadata_resources back to dbt_resources")

    with op.batch_alter_table('metadata_indexing_jobs', schema=None) as batch_op:
        batch_op.drop_column('detected_project_types')

    op.create_table('dbt_resources',
    sa.Column('name', sa.VARCHAR(), nullable=False),
    sa.Column('resource_type', sa.VARCHAR(), nullable=False),
    sa.Column('path', sa.VARCHAR(), nullable=True),
    sa.Column('description', sa.TEXT(), nullable=True),
    sa.Column('raw_data', sqlite.JSON(), nullable=True),
    sa.Column('sql_content', sa.TEXT(), nullable=True),
    sa.Column('source_name', sa.VARCHAR(), nullable=True),
    sa.Column('database', sa.VARCHAR(), nullable=True),
    sa.Column('schema', sa.VARCHAR(), nullable=True),
    sa.Column('columns', sqlite.JSON(), nullable=True),
    sa.Column('depends_on', sqlite.JSON(), nullable=True),
    sa.Column('is_active', sa.BOOLEAN(), nullable=False),
    sa.Column('last_synced_at', sa.DATETIME(), nullable=True),
    sa.Column('data_source_id', sa.VARCHAR(length=36), nullable=False),
    sa.Column('metadata_indexing_job_id', sa.VARCHAR(length=36), nullable=True),
    sa.Column('id', sa.VARCHAR(length=36), nullable=False),
    sa.Column('created_at', sa.DATETIME(), nullable=True),
    sa.Column('updated_at', sa.DATETIME(), nullable=True),
    sa.Column('deleted_at', sa.DATETIME(), nullable=True),
    sa.ForeignKeyConstraint(['data_source_id'], ['data_sources.id'], ),
    sa.ForeignKeyConstraint(['metadata_indexing_job_id'], ['metadata_indexing_jobs.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    with op.batch_alter_table('dbt_resources', schema=None) as batch_op:
        batch_op.create_index('ix_dbt_resources_id', ['id'], unique=1)

    with op.batch_alter_table('metadata_resources', schema=None) as batch_op:
        batch_op.drop_index(batch_op.f('ix_metadata_resources_id'))

    op.drop_table('metadata_resources')
    # ### end Alembic commands ###
